{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992d331f",
   "metadata": {},
   "source": [
    "# Scraping from Indeed and predicting data science salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a59e1",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "The aim of the project is to predict whether salaries for Data Science roles, obtain from a job board, will be higher or lower than the median, based on information extracted from the location and the job title. The models also allowed us to determine which of the features where most important for the salary being higher or lower than the median. London seems to be the location mostly linked to high salaries, while small cities seem to be an important factor when the salary is low. Job titles that refer to manager, lead or engineer are also important features linked to high salaries.\n",
    "The cities included in the search were the 4 main cities of the Republic of Ireland (Dublin, Galway, Cork, Limerick) and the 12 most populous cities in the UK: London, Birmingham, Glasgow, 'Liverpool', Bristol, Manchester, Sheffield, Leeds, Edinburgh, Leicester, Coventry, Bradford.\n",
    "The salaries were all converted to Euro at the exchange rate of 03 September 2021: 1 GBP = 1.17 EUR (03/09/2021).\n",
    "Only yearly salaries were included.\n",
    "Libraries used:\n",
    "-\tBeautifulSoup for scraping\n",
    "-\tPandas for cleaning\n",
    "-\tScikit-learn for modeling\n",
    "-\tMatplotlib for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c915fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import requests\n",
    "import bs4\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499944c7",
   "metadata": {},
   "source": [
    "#### Data collection\n",
    "The data was scraped on ie.indeed.com and uk.indeed.com, using BeautifulSoup, a python library for web scraping. Functions were set up to extract location, company, job title and salary from each parsed job result. The scraper was set to iterate through each city and extract up to 500 max results per city. Through this process, over 10,000 job results were collected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1ad8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to extract items from job results\n",
    "def extract_location_from_result(result):\n",
    "    try:\n",
    "        return result.find('div', class_ = 'companyLocation').text.split(\",\")[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_company_from_result(result):\n",
    "    try:\n",
    "        return result.find('span', class_ = 'companyName').text\n",
    "    except:\n",
    "        try:\n",
    "        #large companies are wrapped in a link to the company page e.g. https://www.indeed.com/cmp/UBS?from=SERP&fromjk=870b00726816cdbd&jcid=1c76c3a36f6c7557&attributionid=serp-linkcompanyname\n",
    "            return result.findChild('a', class_ = 'turnstileLink companyOverviewLink').text\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "def extract_job_from_result(result):\n",
    "    try:\n",
    "        return result.findChild('h2', class_ = 'jobTitle jobTitle-color-purple').text\n",
    "    except:\n",
    "        try:\n",
    "            return result.findChild('h2', class_ = 'jobTitle jobTitle-color-purple jobTitle-newJob').text\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "def extract_salary_from_result(result):\n",
    "    try:\n",
    "        return result.find('span', class_= 'salary-snippet').text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71855591",
   "metadata": {},
   "source": [
    "Note: make sure to use try/except to handle errors and return None if the item is not found\n",
    "\n",
    "Next we iterate through each city and parse the job results via BeautifulSoup. Make sure to set a randomized sleep timer with the sleep() function, to avoid bot detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_results = {}\n",
    "for city in set(['Dublin', 'Galway', 'Cork', 'Limerick', 'London', 'Birmingham', 'Glasgow', 'Liverpool', 'Bristol','Manchester',\n",
    "                'Sheffield', 'Leeds', 'Edinburgh', 'Leicester', 'Coventry', 'Bradford']):\n",
    "    cities_results[city] = []\n",
    "    \n",
    "    if city in ['Dublin', 'Galway', 'Cork', 'Limerick']:\n",
    "        for start in tqdm(range(0, max_results_per_city, 10)):\n",
    "            URL = url_template_irl.format(city,start)\n",
    "            r = requests.get(URL)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            cities_results[city].append(soup.find_all('div', class_ = 'job_seen_beacon'))\n",
    "            sleep(randint(5,12))\n",
    "    else:\n",
    "        for start in tqdm(range(0, max_results_per_city, 10)):\n",
    "            URL = url_template_uk.format(city,start)\n",
    "            r = requests.get(URL)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            cities_results[city].append(soup.find_all('div', class_ = 'job_seen_beacon'))\n",
    "            sleep(randint(5,12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014da7a3",
   "metadata": {},
   "source": [
    "Create a Pandas DataFrame containing the data extracted with the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a982aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['location', 'company','job','salary'])\n",
    "\n",
    "locations = []\n",
    "companies = []\n",
    "jobs = []\n",
    "salaries = []\n",
    "for city,pages in cities_results.items():\n",
    "    for page in pages:\n",
    "        for result in page:\n",
    "            locations.append(extract_location_from_result(result))\n",
    "            companies.append(extract_company_from_result(result))\n",
    "            jobs.append(extract_job_from_result(result))\n",
    "            salaries.append(extract_salary_from_result(result))\n",
    "            \n",
    "df.location = locations\n",
    "df.company = companies\n",
    "df.job = jobs\n",
    "df.salary = salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ed2db",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "The data collected required cleaning:\n",
    "-\tLocation data was  not uniform, as some indicated ‘remote’, others contained post codes\n",
    "-\tMost job results did not contain a salary estimate\n",
    "-\tMany of the jobs came out to be duplicated\n",
    "-\tSelected only yearly salaries\n",
    "-\tConverted salaries to numerical format, eliminating hyphens and currency symbols, and calculating midpoint when a salary range was provided\n",
    "After cleaning, the remaining dataset contained 264 observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fa85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with null salaries\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean locations\n",
    "def clean_location(x):\n",
    "    if 'West Midlands' in x:\n",
    "        return 'West Midlands'\n",
    "    elif 'Leamington Spa' in x:\n",
    "        return 'Leamington Spa'\n",
    "    elif 'Birmingham' in x:\n",
    "        return 'Birmingham'\n",
    "    elif 'Longbridge' in x:\n",
    "        return 'Longbridge'\n",
    "    elif 'London' in x:\n",
    "        return 'London'\n",
    "    elif 'Leeds' in x:\n",
    "        return 'Leeds'\n",
    "    elif 'St Albans' in x:\n",
    "        return 'St Albans'\n",
    "    elif 'Uxbridge' in x:\n",
    "        return 'Uxbridge'\n",
    "    elif 'Epsom' in x:\n",
    "        return 'Epsom'\n",
    "    elif 'Glasgow' in x:\n",
    "        return 'Glasgow'\n",
    "    elif 'Manchester' in x:\n",
    "        return 'Manchester'\n",
    "    elif 'Bristol' in x:\n",
    "        return 'Bristol'\n",
    "    elif 'Bath' in x:\n",
    "        return 'Bath'\n",
    "    elif 'Blackburn' in x:\n",
    "        return 'Blackburn'\n",
    "    elif 'Salford' in x:\n",
    "        return 'Salford'\n",
    "    elif 'Edinburgh' in x:\n",
    "        return 'Edinburgh'\n",
    "    elif 'Dublin' in x:\n",
    "        return 'Dublin'\n",
    "    elif 'Chester' in x:\n",
    "        return 'Chester'\n",
    "    elif 'Huddersfield' in x:\n",
    "        return 'Huddersfield'\n",
    "    elif 'Sheffield' in x:\n",
    "        return 'Sheffield'\n",
    "    elif 'Altrincham' in x:\n",
    "        return 'Altrincham'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df.location = df.location.apply(clean_location)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "df.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86237966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean salaries and convert to Euro\n",
    "def clean_salary(data):\n",
    "    eur = 1.17\n",
    "    if 'a year' in data:\n",
    "        if '£' in data:\n",
    "            if '-' not in data:\n",
    "                return float(data.replace('£', '').replace(' a year','').replace(',','')) * eur\n",
    "            #averaging entries containing a salary range\n",
    "            else:\n",
    "                num1 =  float(data.replace('£', '').replace(' a year','').replace(',','').split()[0])\n",
    "                num2 =  float(data.replace('£', '').replace(' a year','').replace('-','').replace(',','').split()[1])\n",
    "                return float((num2+num1)/2) * eur\n",
    "        else:\n",
    "            if '-' not in data:\n",
    "                return float(data.replace('€', '').replace(' a year','').replace(',',''))\n",
    "            else:\n",
    "                num1 =  float(data.replace('€', '').replace(' a year','').replace(',','').split()[0])\n",
    "                num2 =  float(data.replace('€', '').replace(' a year','').replace('-','').replace(',','').split()[1])\n",
    "                return float((num2+num1)/2)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to csv\n",
    "df.to_csv('salaries_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3cc22",
   "metadata": {},
   "source": [
    "#### Models\n",
    "The aim of the models was to predict high and low salaries. The threshold of choice was the median of the salaries, above which it would be assigned the high salary class, otherwise the low salary class. Logistic Regression and Random Forest Classifier were the models of choice.\n",
    "Different models were evaluated using different features or a combination of them:\n",
    "1.\tFitting the models only using locations as features\n",
    "2.\tAdding a variable for job titles containing ‘manager’ and ‘senior’\n",
    "3.\tAdding additional features extracted from job title, selecting the top 10 recurring words\n",
    "4.\tTuning the models around accuracy\n",
    "5.\tOptimizing models to increase precision to minimize false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a logistic regression with hyperparameters tuning via GridSearchCV\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "params = {'C': np.logspace(-4, 4,10),\n",
    "          'penalty': ['l1', 'l2'],\n",
    "          'fit_intercept': [True, False],\n",
    "         'solver':['saga']}\n",
    "\n",
    "gs = GridSearchCV(estimator=logreg,\n",
    "                  param_grid=params,\n",
    "                  cv=5,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=-2,\n",
    "                  verbose=1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "print('Best Parameters:')\n",
    "print(gs.best_params_)\n",
    "print('Best estimator C:')\n",
    "print(gs.best_estimator_.C)\n",
    "print('Best estimator mean cross validated training score:')\n",
    "print(gs.best_score_)\n",
    "print('Best estimator score on the full training set:')\n",
    "print(gs.score(X_train, y_train))\n",
    "print('Best estimator score on the test set:')\n",
    "print(gs.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the model coefficients in a dataframe\n",
    "df_coef = pd.DataFrame(gs.best_estimator_.coef_[0], index=X_train.columns,\n",
    "                       columns=['coefficients'])\n",
    "# calculate the absolute values of the coefficients\n",
    "df_coef['coef_abs'] = df_coef.coefficients.abs()\n",
    "df_coef\n",
    "\n",
    "# plot the magnitude of the coefficients\n",
    "plt.figure(figsize=(8,12))\n",
    "df_coef['coefficients'].sort_values().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "params_rf = {'criterion': ['gini','entropy'],\n",
    "          'max_depth': [None, 1,3,5,10,20],\n",
    "          'max_leaf_nodes': list(range(2, 21,3)) + [None],\n",
    "          \"min_samples_leaf\":list(range(1,10,2))}\n",
    "\n",
    "gs_rf = GridSearchCV(estimator=rf,\n",
    "                  param_grid=params_rf,\n",
    "                  cv=5,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=-2,\n",
    "                  verbose=1)\n",
    "\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "# extract the grid search results\n",
    "\n",
    "print('Best Parameters:')\n",
    "print(gs_rf.best_params_)\n",
    "print('Best estimator mean cross validated training score:')\n",
    "print(gs_rf.best_score_)\n",
    "print('Best estimator score on the full training set:')\n",
    "print(gs_rf.score(X_train, y_train))\n",
    "print('Best estimator score on the test set:')\n",
    "print(gs_rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ad95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract important features and plot\n",
    "feats = {feat:val for feat, val in zip(X.columns,gs_rf.best_estimator_.feature_importances_)}\n",
    "feats = pd.DataFrame(data = feats.values(), index = X.columns, columns = ['feat_importance']).sort_values(by = 'feat_importance')\n",
    "plt.figure(figsize = (8,12))\n",
    "plt.barh(feats.index,feats.feat_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ebe4b3",
   "metadata": {},
   "source": [
    "Visualize relevant metrics to evaluate the models:\n",
    "- confusion matrix\n",
    "- classification report\n",
    "- Precision-recall curve\n",
    "- ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9649b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_test, prob_test['prediction'], labels=[1, 0], figsize=(6, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, prob_test['prediction']))\n",
    "\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.63      0.93      0.75        41\n",
    "#            1       0.85      0.44      0.58        39\n",
    "\n",
    "#     accuracy                           0.69        80\n",
    "#    macro avg       0.74      0.68      0.66        80\n",
    "# weighted avg       0.74      0.69      0.67        80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d95ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_test_rf = gs_rf.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "skplt.metrics.plot_precision_recall(y_test, probabilities_test_rf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_test_rf = gs_rf.best_estimator_.predict_proba(X_test)skplt.metrics.plot_roc(y_test, probabilities_test_rf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed40455",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "The highest performing model was able to predict new data accurately around 70% of the time. This is a significant improvement from the baseline, which stood at 50%. The baseline of 50% is given by the majority class within the data. In this case, being 50.7% of the data categorized as low salaries, then guessing low salary on all the data would be our baseline.\n",
    "Additionally, we are able to identify which factors are the most important for identifying high or low salaries, e.g.: whether the job is in London or in a small city and whether the job title refers to a manager, lead or engineer.\n",
    "A consideration must be made due to the really low number of observations compared to the total of job results scraped from Indeed. Only few companies decide to advertise a salary estimate, therefore it cannot be guaranteed that the salaries extracted truly represent the general job market. Also, the job market in the Data Science field is in rapid evolution and the boundaries between different job positions are not well defined. Moreover, additional data could be taken in consideration, such as extracting information from job descriptions, to increase the performance of the models. Lastly, the tradeoff between model accuracy and minimization of false positives should be kept in mind when evaluating the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
